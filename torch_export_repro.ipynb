{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003dfd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from nemo.collections.asr.models import EncDecMultiTaskModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ba6c5a",
   "metadata": {},
   "source": [
    "## Init the model and change the decoding strategy to greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e364098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import DictConfig\n",
    "import torch\n",
    "\n",
    "torch.autograd.set_grad_enabled(False)\n",
    "\n",
    "model = EncDecMultiTaskModel.from_pretrained(\"nvidia/canary-180m-flash\").eval().cpu()\n",
    "decoding_strategy = DictConfig({\n",
    "        \"strategy\": \"greedy\",\n",
    "        \"return_best_hypothesis\": True,\n",
    "    },)\n",
    "model.change_decoding_strategy(decoding_strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccbd5ff",
   "metadata": {},
   "source": [
    "## Prepare inputs for the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2271fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "waveform, sr = torchaudio.load(\"./audio_2.mp3\") # BTW this could be a random array as well, if you don't have any audio files at hand\n",
    "# Resample to 16kHz\n",
    "target_sr = 16_000\n",
    "if sr != target_sr:\n",
    "    resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)\n",
    "    waveform = resampler(waveform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c3cb25",
   "metadata": {},
   "source": [
    "## Utility functions that are used within the Nemo's EncDecMultiTaskModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f92ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def lens_to_mask(lens, max_length):\n",
    "    \"\"\"\n",
    "    Create a mask from a tensor of lengths.\n",
    "    \"\"\"\n",
    "    batch_size = lens.shape[0]\n",
    "    arange = torch.arange(max_length, device=lens.device)\n",
    "    mask = arange.expand(batch_size, max_length) < lens.unsqueeze(1)\n",
    "    return mask\n",
    "\n",
    "def mask_padded_tokens(tokens, pad_id):\n",
    "    mask = tokens != pad_id\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1364ce6c",
   "metadata": {},
   "source": [
    "## Prep input for the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c58f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "kwargs = {\n",
    "    \"input_signal\": waveform.detach(),\n",
    "    \"length\": torch.tensor([waveform.shape[-1]], dtype=torch.int32).detach(),\n",
    "}\n",
    "\n",
    "preprocessor_output = model.preprocessor.get_features(**kwargs)\n",
    "log_mel = preprocessor_output[0]\n",
    "log_mel_length = preprocessor_output[1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoded, encoded_len = model.encoder.forward_for_export(audio_signal=log_mel, length=log_mel_length)\n",
    "    enc_states = encoded.permute(0,2,1)\n",
    "    enc_states = model.encoder_decoder_proj(enc_states).detach()\n",
    "    enc_mask = lens_to_mask(encoded_len, enc_states.shape[1]).to(enc_states.dtype).detach()\n",
    "    input_ids = torch.tensor([ 7,  4, 16, 62, 62,  5,  9, 11, 13], dtype=torch.int64).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8fe237",
   "metadata": {},
   "source": [
    "## Define a wrapper and export the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be7549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWrapper(torch.nn.Module):\n",
    "    def __init__(self, embedding, decoder, classifier):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.decoder = decoder\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        decoder_input_ids=None,\n",
    "        encoder_hidden_states=None,\n",
    "    ):\n",
    "        input_ids = decoder_input_ids\n",
    "        encoder_input_mask = lens_to_mask(\n",
    "            torch.tensor([encoder_hidden_states.shape[1]], dtype=torch.int32),\n",
    "            encoder_hidden_states.shape[1],\n",
    "        ).to(encoder_hidden_states.dtype)\n",
    "        logits, decoder_mems_list = self._one_step_forward(\n",
    "            input_ids,\n",
    "            encoder_hidden_states,\n",
    "            encoder_input_mask,\n",
    "            None,  # no past mems yet\n",
    "            0,\n",
    "        )\n",
    "\n",
    "        next_tokens = torch.argmax(logits[:, -1], dim=-1)\n",
    "        input_ids = torch.cat((input_ids, next_tokens.unsqueeze(1)), dim=-1)\n",
    "        return input_ids\n",
    "\n",
    "    def _one_step_forward(\n",
    "        self,\n",
    "        decoder_input_ids=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_input_mask=None,\n",
    "        decoder_mems_list=None,\n",
    "        pos=0,\n",
    "    ):\n",
    "        decoder_hidden_states = self.embedding.forward(decoder_input_ids, start_pos=pos)\n",
    "        decoder_input_mask = mask_padded_tokens(decoder_input_ids, 2).float()\n",
    "\n",
    "        if encoder_hidden_states is not None:\n",
    "            decoder_mems_list = self.decoder.forward(\n",
    "                decoder_hidden_states,\n",
    "                decoder_input_mask,\n",
    "                encoder_hidden_states,\n",
    "                encoder_input_mask,\n",
    "                decoder_mems_list,\n",
    "                return_mems=True,\n",
    "            )\n",
    "        else:\n",
    "            decoder_mems_list = self.decoder.forward(\n",
    "                decoder_hidden_states,\n",
    "                decoder_input_mask,\n",
    "                decoder_mems_list,\n",
    "                return_mems=True,\n",
    "            )\n",
    "\n",
    "        logits = self.classifier.forward(hidden_states=decoder_mems_list[-1][:, -1:])\n",
    "        return logits, decoder_mems_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba42c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = model.decoding.transformer_decoder.decoder\n",
    "embedding = model.decoding.transformer_decoder.embedding\n",
    "classifier = model.log_softmax.mlp\n",
    "\n",
    "wrapper = DecoderWrapper(\n",
    "    decoder=decoder,\n",
    "    embedding=embedding,\n",
    "    classifier=classifier,\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a512cfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.export import Dim\n",
    "\n",
    "dynamic_shapes = {\n",
    "    \"decoder_input_ids\": {\n",
    "        1: Dim(\"encoder_hidden_state_len\", min=1, max=3000)\n",
    "    },  # Not sure if that is the number we're looking for, but essentially it should be up to 40s\n",
    "    \"encoder_hidden_states\": {1: Dim(\"decode_input_ids\", min=1, max=1024)},\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    exported = torch.export.export(\n",
    "        wrapper,\n",
    "        args=(input_ids, enc_states),\n",
    "        strict=False,\n",
    "        dynamic_shapes=dynamic_shapes\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4075ade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "exported.module().forward(input_ids, enc_states,)[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a047624",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.export.save(exported, './bin/nemo_decoder.pt2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
